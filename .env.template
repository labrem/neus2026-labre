# Project paths
PROJECT_ROOT=/path/to/openmath
MODELS_DIR=/path/to/.models
PYTHONPATH=/path/to/openmath

# vLLM settings
# Port for vLLM server (used for HyDE generation with Qwen2.5-Math)
# VLLM_API_URL is constructed as http://localhost:${VLLM_API_PORT}/v1
VLLM_API_PORT=9000
VLLM_API_KEY=""

# Fraction of GPU memory to use for model weights and KV cache (default: 0.7)
# Lower this value if you encounter "Free memory on device is less than desired" errors
# Typical values: 0.5 for shared GPU, 0.7-0.9 for dedicated GPU
VLLM_GPU_MEMORY_UTILIZATION=0.5

# Disable CUDA graphs for stability on newer GPUs (NVIDIA GB10/Blackwell)
# Set to true if you experience "Engine core died unexpectedly" errors
# Trade-off: slightly slower inference but more stable
VLLM_ENFORCE_EAGER=true

# Logging
LOG_LEVEL=INFO

# Executor settings
# Maximum execution time in seconds for LLM-generated code (default: 10)
# Increase for complex symbolic computations, decrease for faster timeout
EXECUTOR_TIMEOUT_SECONDS=10

# Retriever settings
# Maximum number of OpenMath symbols to retrieve per query (default: 10)
# Higher values provide more context but increase prompt length
RETRIEVER_MAX_SYMBOLS=10

# Minimum keyword match score for symbol retrieval (default: 1)
# Higher values require more keyword matches, reducing noise
RETRIEVER_MIN_SCORE=1

# Only return symbols with SymPy mappings (default: true)
# Set to false to include all symbols regardless of executability
RETRIEVER_REQUIRE_SYMPY=true

# Comparator settings
# Tolerance for numerical comparison (default: 1e-9)
# Use 1e-4 for ~4 decimal place matching, 1e-6 for ~6 decimal places
COMPARATOR_TOLERANCE=1e-9

# Ollama API - direct connection to local Ollama service
# Note: Port 11435 (Caddy proxy) was not responding; using direct port 11434
OLLAMA_API_URL="http://localhost:11434/v1"
OLLAMA_API_KEY=""

# vLLM Reranker Server (Qwen3-Reranker via pooling runner)
# Separate from main vLLM inference server to run cross-encoder models
# Start with:
#   ./scripts/start_vllm_server.sh --model Qwen/Qwen3-Reranker-0.6B --runner pooling --port 9001 \
#     --hf-overrides '{"architectures":["Qwen3ForSequenceClassification"],"classifier_from_token":["no","yes"],"is_original_qwen3_reranker":true}'
VLLM_RERANKER_PORT=9001
VLLM_RERANKER_URL=http://localhost:9001
VLLM_RERANKER_MODEL=Qwen/Qwen3-Reranker-0.6B
